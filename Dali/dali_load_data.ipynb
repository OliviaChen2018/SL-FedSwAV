{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84fbec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia.dali.pipeline import pipeline_def\n",
    "import nvidia.dali.types as types\n",
    "import nvidia.dali.fn as fn\n",
    "from nvidia.dali.plugin.pytorch import DALIGenericIterator\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6be0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 加载cifar10数据集 ####\n",
    "def load_cifar10(batch_size, train=True, root='/data/cifar10'):\n",
    "    '''该函数返回的结果与torchvision.datasets.CIFAR10()函数取self.data和self.targets返回的结果相同''''\n",
    "    base_folder = 'cifar-10-batches-py'\n",
    "    train_list = [\n",
    "        ['data_batch_1', 'c99cafc152244af753f735de768cd75f'],\n",
    "        ['data_batch_2', 'd4bba439e000b95fd0a9bffe97cbabec'],\n",
    "        ['data_batch_3', '54ebc095f3ab1f0389bbae665268c751'],\n",
    "        ['data_batch_4', '634d18415352ddfa80567beed471001a'],\n",
    "        ['data_batch_5', '482c414d41f54cd18b22e5b47cb7c3cb'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test_batch', '40351d587109b95175f43aff81a1287e'],\n",
    "    ]\n",
    "    \n",
    "    if train:\n",
    "        downloaded_list = train_list\n",
    "    else:\n",
    "        downloaded_list = test_list\n",
    "\n",
    "    data = []\n",
    "    targets = []\n",
    "    for file_name, checksum in downloaded_list:\n",
    "        file_path = os.path.join(root, base_folder, file_name)\n",
    "        with open(file_path, 'rb') as f:\n",
    "            if sys.version_info[0] == 2:\n",
    "                entry = pickle.load(f)\n",
    "            else:\n",
    "                entry = pickle.load(f, encoding='latin1')\n",
    "            data.append(entry['data'])\n",
    "            if 'labels' in entry:\n",
    "                targets.extend(entry['labels'])\n",
    "            else:\n",
    "                targets.extend(entry['fine_labels'])\n",
    "\n",
    "    data = np.vstack(data).reshape(-1, 3, 32, 32)\n",
    "    data = data.transpose((0, 2, 3, 1))  # convert to HWC\n",
    "    targets = np.vstack(targets)\n",
    "#     np.save(\"cifar.npy\", data)\n",
    "#     data = np.load('cifar.npy')  # to serialize, increase locality\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d5698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"../data/\"\n",
    "batch_size = 256\n",
    "data, targets = load_cifar(batch_size, train=True, root=image_dir)\n",
    "\n",
    "# @pipeline_def\n",
    "# def simple_pipeline():\n",
    "#     jpegs, labels = fn.readers.file(file_root=image_dir)\n",
    "#     images = fn.decoders.image(jpegs, device='cpu')\n",
    "\n",
    "#     return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98295f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"data的size为:{data.shape}\") # [50000, 32, 32, 3]\n",
    "print(f\"targets的size为:{targets.shape}\") # [50000, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af606325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data(training_data, labels, num_client, num_class, partition = 'noniid', beta=0.4): \n",
    "    '''按照Dirichlet分布划分原始数据集 '''\n",
    "    # 参数num_client表示client的数量\n",
    "    # training_data和labels是numpy数组\n",
    "    training_data_subset_list = []\n",
    "    training_label_subset_list = []\n",
    "    if partition == \"homo\" or partition == \"iid\":\n",
    "        idxs = np.random.permutation(N)   #在训练集的条数范围内生成随机序列\n",
    "        batch_idxs = np.array_split(idxs, num_client)\n",
    "        net_dataidx_map = {i: batch_idxs[i] for i in range(num_client)}\n",
    "\n",
    "    elif partition == \"noniid-labeldir\" or partition == \"noniid\":\n",
    "        min_size = 0 \n",
    "        min_require_size = 10   # 每个client至少要有10条数据\n",
    "        K = num_class\n",
    "            # min_require_size = 100\n",
    "\n",
    "        N = labels.shape[0]\n",
    "        net_dataidx_map = {}  #用于存放每个client拥有的样本的idx数组\n",
    "\n",
    "        #min_size表示所有client中样本数量最少的client对应的样本数量。如果存在某个client的样本数量没达到min_require_size，则继续为client分配样本。\n",
    "        while min_size < min_require_size:\n",
    "            idx_batch = [[] for _ in range(num_client)]  # idx_batch存放num_client个client对应的样本idx\n",
    "            for k in range(K): #遍历所有类别，将每个类别按Dirichlet分布的比例分配给各个client。\n",
    "                idx_k = np.where(labels == k)[0]  #idx_k表示训练集中label为k的所有样本的idx集合\n",
    "                np.random.shuffle(idx_k) #上面选出来的idx是按顺序的，现在把顺序打乱。\n",
    "                proportions = np.random.dirichlet(np.repeat(beta, num_client)) \n",
    "                #proportions的长度为num_client\n",
    "                proportions = np.array([p * (len(idx_j) < N / num_client) for p, idx_j in zip(proportions, idx_batch)])  # 取出第j个client拥有的所有sample下标和第j个client的idx\n",
    "                proportions = proportions / proportions.sum() #将剩下的client的划分比例重新归一化\n",
    "                proportions = (np.cumsum(proportions) * len(idx_k)).astype(int)[:-1] #\n",
    "                idx_batch = [idx_j + idx.tolist() for idx_j, idx in zip(idx_batch, np.split(idx_k, proportions))]  #为第j个client分配类别k的样本\n",
    "                min_size = min([len(idx_j) for idx_j in idx_batch]) #min_size表示所有client中样本数量最少的client对应的样本数量\n",
    "                # if K == 2 and num_client <= 10:\n",
    "                #     if np.min(proportions) < 200:\n",
    "                #         min_size = 0\n",
    "                #         break\n",
    "\n",
    "\n",
    "        for j in range(num_client):\n",
    "            #分配完之后，由于idx_batch中的样本idx是按类别顺序存放的，所以要打乱。\n",
    "            np.random.shuffle(idx_batch[j]) \n",
    "            net_dataidx_map[j] = idx_batch[j] # 用net_dataidx_map记录每个client拥有的样本。\n",
    "            # 封装为dataloader\n",
    "            training_data_subset = training_data[idx_batch[j]]\n",
    "            training_label_subset = labels[idx_batch[j]]\n",
    "            print(f\"labels的维度为:{labels.shape}\")\n",
    "            print(f\"idx_batch[{j}]:{idx_batch[j]}\")\n",
    "            training_data_subset_list.append(training_data_subset)\n",
    "            training_label_subset_list.append(training_label_subset)\n",
    "#     print(net_dataidx_map)\n",
    "    #traindata_cls_counts：数据分布情况（每个client拥有的所有类别及其数量）\n",
    "    traindata_cls_counts = record_net_data_stats(labels, net_dataidx_map) \n",
    "\n",
    "    return training_data_subset_list, training_label_subset_list, traindata_cls_counts\n",
    "\n",
    "def record_net_data_stats(y_train, net_dataidx_map):\n",
    "    '''用于记录每个client的数据分布(拥有的所有样本类别，及该类别出现的次数)'''\n",
    "    net_cls_counts = {}\n",
    "\n",
    "    for net_i, dataidx in net_dataidx_map.items(): # dict.items()返回(key, value)元组组成的列表\n",
    "    # net_i表示第i个client, dataidx为其拥有的样本idx\n",
    "        unq, unq_cnt = np.unique(y_train[dataidx], return_counts=True) #返回unique的类别数组\n",
    "        tmp = {unq[i]: unq_cnt[i] for i in range(len(unq))} # 字典,存放第i个client拥有的类别及其数量\n",
    "        net_cls_counts[net_i] = tmp #字典，存放所有client的类别信息\n",
    "\n",
    "    data_list=[]\n",
    "    for net_id, data in net_cls_counts.items(): # net_id表示client编号，data表示该client拥有的类别的次数信息\n",
    "        n_total=0\n",
    "        for class_id, n_data in data.items(): # class_id表示类别编号，n_data表示该类别在该client中的出现次数\n",
    "            n_total += n_data  # 计算该client拥有的数据条数\n",
    "        data_list.append(n_total) #data_list保存每个client拥有的数据条数\n",
    "    print('mean:', np.mean(data_list)) #打印每个client的平均数据条数和方差，以显示异质程度\n",
    "    print('std:', np.std(data_list))\n",
    "\n",
    "    return net_cls_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d675dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_list, training_label_list, traindata_cls_counts = partition_data(data,targets,num_client=10,num_class=10, partition = 'noniid', beta=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e89588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data_list[0].shape) #[(item_num, 32, 32, 3)*10]\n",
    "# print(training_label_list[0].shape) # [(item_num, 1)*10]\n",
    "# print(training_data_list[0].shape==training_label_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb369b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建num_client个pipeline.\n",
    "# 在pipeline中定义数据增强的流程\n",
    "\n",
    "CIFAR10_MEAN=[0.49139968 * 255., 0.48215827 * 255., 0.44653124 * 255.]\n",
    "CIFAR10_STD=[0.24703233 * 255., 0.24348505 * 255., 0.26158768 * 255.]\n",
    "@pipeline_def(num_threads=4, device_id=0)\n",
    "def get_dali_pipeline(images, labels):\n",
    "#     images, labels = fn.readers.file(\n",
    "#         file_root=images_dir, random_shuffle=True, name=\"Reader\")\n",
    "    # decode data on the GPU\n",
    "    images = fn.decoders.image_random_crop(\n",
    "        images, device=\"gpu\", output_type=types.RGB, random_area=[0.14,1])\n",
    "    # the rest of processing happens on the GPU as well\n",
    "    images = fn.resize(images, resize_x=224, resize_y=224)\n",
    "    \n",
    "    images = fn.random.coin_flip(probability=0.5)\n",
    "#     images = fn.crop_mirror_normalize(\n",
    "#         images,\n",
    "#         device=\"gpu\",\n",
    "#         crop_h=224,\n",
    "#         crop_w=224,\n",
    "#         mean=CIFAR10_MEAN,\n",
    "#         std=CIFAR10_STD)\n",
    "#     transforms.RandomHorizontalFlip(p=0.5),\n",
    "#                 transforms.Compose(color_transform),\n",
    "#                 transforms.ToTensor(),\n",
    "#                 transforms.Normalize(mean=mean, std=std)])\n",
    "#             ] * nmb_crops[i])\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bafd619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义dataloader\n",
    "train_data = DALIGenericIterator(\n",
    "    [get_dali_pipeline(training_data_list[0], training_label_list[0], batch_size=256)],\n",
    "    ['data', 'label']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a337ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_data):\n",
    "    x, y = data[0]['data'], data[0]['label']\n",
    "    print(f\"size of x is :{x.size()}\")\n",
    "    print(f\"size of y is: {y.size()}\")\n",
    "#     pred = model(x)\n",
    "#     loss = loss_func(pred, y)\n",
    "#     backward(loss, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedprocon",
   "language": "python",
   "name": "fedprocon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
