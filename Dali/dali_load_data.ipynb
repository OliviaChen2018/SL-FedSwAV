{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f84fbec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia.dali.pipeline import pipeline_def\n",
    "import nvidia.dali.types as types\n",
    "import nvidia.dali.fn as fn\n",
    "from nvidia.dali.plugin.pytorch import DALIGenericIterator\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb6be0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 加载cifar10数据集 ####\n",
    "def load_cifar10(batch_size, train=True, root='/data/cifar10'):\n",
    "    '''该函数返回的结果与torchvision.datasets.CIFAR10()函数取self.data和self.targets返回的结果相同'''\n",
    "    base_folder = 'cifar-10-batches-py'\n",
    "    train_list = [\n",
    "        ['data_batch_1', 'c99cafc152244af753f735de768cd75f'],\n",
    "        ['data_batch_2', 'd4bba439e000b95fd0a9bffe97cbabec'],\n",
    "        ['data_batch_3', '54ebc095f3ab1f0389bbae665268c751'],\n",
    "        ['data_batch_4', '634d18415352ddfa80567beed471001a'],\n",
    "        ['data_batch_5', '482c414d41f54cd18b22e5b47cb7c3cb'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test_batch', '40351d587109b95175f43aff81a1287e'],\n",
    "    ]\n",
    "    \n",
    "    if train:\n",
    "        downloaded_list = train_list\n",
    "    else:\n",
    "        downloaded_list = test_list\n",
    "\n",
    "    data = []\n",
    "    targets = []\n",
    "    for file_name, checksum in downloaded_list:\n",
    "        file_path = os.path.join(root, base_folder, file_name)\n",
    "        with open(file_path, 'rb') as f:\n",
    "            if sys.version_info[0] == 2:\n",
    "                entry = pickle.load(f)\n",
    "            else:\n",
    "                entry = pickle.load(f, encoding='latin1')\n",
    "            data.append(entry['data'])\n",
    "            if 'labels' in entry:\n",
    "                targets.extend(entry['labels'])\n",
    "            else:\n",
    "                targets.extend(entry['fine_labels'])\n",
    "\n",
    "    data = np.vstack(data).reshape(-1, 3, 32, 32)\n",
    "    data = data.transpose((0, 2, 3, 1))  # convert to HWC\n",
    "    targets = np.vstack(targets)\n",
    "#     np.save(\"cifar.npy\", data)\n",
    "#     data = np.load('cifar.npy')  # to serialize, increase locality\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87d5698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"../data/\"\n",
    "batch_size = 256\n",
    "data, targets = load_cifar10(batch_size, train=True, root=image_dir)\n",
    "\n",
    "# @pipeline_def\n",
    "# def simple_pipeline():\n",
    "#     jpegs, labels = fn.readers.file(file_root=image_dir)\n",
    "#     images = fn.decoders.image(jpegs, device='cpu')\n",
    "\n",
    "#     return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98295f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"data的size为:{data.shape}\") # [50000, 32, 32, 3]\n",
    "print(f\"targets的size为:{targets.shape}\") # [50000, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af606325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data(training_data, labels, num_client, num_class, partition = 'noniid', beta=0.4): \n",
    "    '''按照Dirichlet分布划分原始数据集 '''\n",
    "    # 参数num_client表示client的数量\n",
    "    # training_data和labels是numpy数组\n",
    "    training_data_subset_list = []\n",
    "    training_label_subset_list = []\n",
    "    if partition == \"homo\" or partition == \"iid\":\n",
    "        idxs = np.random.permutation(N)   #在训练集的条数范围内生成随机序列\n",
    "        batch_idxs = np.array_split(idxs, num_client)\n",
    "        net_dataidx_map = {i: batch_idxs[i] for i in range(num_client)}\n",
    "\n",
    "    elif partition == \"noniid-labeldir\" or partition == \"noniid\":\n",
    "        min_size = 0 \n",
    "        min_require_size = 10   # 每个client至少要有10条数据\n",
    "        K = num_class\n",
    "            # min_require_size = 100\n",
    "\n",
    "        N = labels.shape[0]\n",
    "        net_dataidx_map = {}  #用于存放每个client拥有的样本的idx数组\n",
    "\n",
    "        #min_size表示所有client中样本数量最少的client对应的样本数量。如果存在某个client的样本数量没达到min_require_size，则继续为client分配样本。\n",
    "        while min_size < min_require_size:\n",
    "            idx_batch = [[] for _ in range(num_client)]  # idx_batch存放num_client个client对应的样本idx\n",
    "            for k in range(K): #遍历所有类别，将每个类别按Dirichlet分布的比例分配给各个client。\n",
    "                idx_k = np.where(labels == k)[0]  #idx_k表示训练集中label为k的所有样本的idx集合\n",
    "                np.random.shuffle(idx_k) #上面选出来的idx是按顺序的，现在把顺序打乱。\n",
    "                proportions = np.random.dirichlet(np.repeat(beta, num_client)) \n",
    "                #proportions的长度为num_client\n",
    "                proportions = np.array([p * (len(idx_j) < N / num_client) for p, idx_j in zip(proportions, idx_batch)])  # 取出第j个client拥有的所有sample下标和第j个client的idx\n",
    "                proportions = proportions / proportions.sum() #将剩下的client的划分比例重新归一化\n",
    "                proportions = (np.cumsum(proportions) * len(idx_k)).astype(int)[:-1] #\n",
    "                idx_batch = [idx_j + idx.tolist() for idx_j, idx in zip(idx_batch, np.split(idx_k, proportions))]  #为第j个client分配类别k的样本\n",
    "                min_size = min([len(idx_j) for idx_j in idx_batch]) #min_size表示所有client中样本数量最少的client对应的样本数量\n",
    "                # if K == 2 and num_client <= 10:\n",
    "                #     if np.min(proportions) < 200:\n",
    "                #         min_size = 0\n",
    "                #         break\n",
    "\n",
    "\n",
    "        for j in range(num_client):\n",
    "            #分配完之后，由于idx_batch中的样本idx是按类别顺序存放的，所以要打乱。\n",
    "            np.random.shuffle(idx_batch[j]) \n",
    "            net_dataidx_map[j] = idx_batch[j] # 用net_dataidx_map记录每个client拥有的样本。\n",
    "            # 封装为dataloader\n",
    "            training_data_subset = training_data[idx_batch[j]]\n",
    "            training_label_subset = labels[idx_batch[j]]\n",
    "#             print(f\"labels的维度为:{labels.shape}\")\n",
    "#             print(f\"idx_batch[{j}]:{idx_batch[j]}\")\n",
    "            training_data_subset_list.append(training_data_subset)\n",
    "            training_label_subset_list.append(training_label_subset)\n",
    "#     print(net_dataidx_map)\n",
    "    #traindata_cls_counts：数据分布情况（每个client拥有的所有类别及其数量）\n",
    "    traindata_cls_counts = record_net_data_stats(labels, net_dataidx_map) \n",
    "\n",
    "    return training_data_subset_list, training_label_subset_list, traindata_cls_counts\n",
    "\n",
    "def record_net_data_stats(y_train, net_dataidx_map):\n",
    "    '''用于记录每个client的数据分布(拥有的所有样本类别，及该类别出现的次数)'''\n",
    "    net_cls_counts = {}\n",
    "\n",
    "    for net_i, dataidx in net_dataidx_map.items(): # dict.items()返回(key, value)元组组成的列表\n",
    "    # net_i表示第i个client, dataidx为其拥有的样本idx\n",
    "        unq, unq_cnt = np.unique(y_train[dataidx], return_counts=True) #返回unique的类别数组\n",
    "        tmp = {unq[i]: unq_cnt[i] for i in range(len(unq))} # 字典,存放第i个client拥有的类别及其数量\n",
    "        net_cls_counts[net_i] = tmp #字典，存放所有client的类别信息\n",
    "\n",
    "    data_list=[]\n",
    "    for net_id, data in net_cls_counts.items(): # net_id表示client编号，data表示该client拥有的类别的次数信息\n",
    "        n_total=0\n",
    "        for class_id, n_data in data.items(): # class_id表示类别编号，n_data表示该类别在该client中的出现次数\n",
    "            n_total += n_data  # 计算该client拥有的数据条数\n",
    "        data_list.append(n_total) #data_list保存每个client拥有的数据条数\n",
    "    print('mean:', np.mean(data_list)) #打印每个client的平均数据条数和方差，以显示异质程度\n",
    "    print('std:', np.std(data_list))\n",
    "\n",
    "    return net_cls_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d675dcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 5000.0\n",
      "std: 2147.5605695765603\n"
     ]
    }
   ],
   "source": [
    "training_data_list, training_label_list, traindata_cls_counts = partition_data(data,targets,num_client=10,num_class=10, partition = 'noniid', beta=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e89588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data_list[0].shape) #[(item_num, 32, 32, 3)*10]\n",
    "# print(training_label_list[0].shape) # [(item_num, 1)*10]\n",
    "# print(training_data_list[0].shape==training_label_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1eb369b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建num_client个pipeline.\n",
    "# 在pipeline中定义数据增强的流程\n",
    "\n",
    "CIFAR10_MEAN=[0.49139968 * 255., 0.48215827 * 255., 0.44653124 * 255.]\n",
    "CIFAR10_STD=[0.24703233 * 255., 0.24348505 * 255., 0.26158768 * 255.]\n",
    "@pipeline_def(num_threads=4, device_id=0)\n",
    "def get_dali_pipeline(images, labels):\n",
    "#     images, labels = fn.readers.file(\n",
    "#         file_root=images_dir, random_shuffle=True, name=\"Reader\")\n",
    "    # decode data on the GPU\n",
    "#     images = fn.decoders.image_random_crop(\n",
    "#         images, device=\"gpu\", output_type=types.RGB, random_area=[0.14,1])\n",
    "#     # the rest of processing happens on the GPU as well\n",
    "#     images = fn.resize(images, resize_x=224, resize_y=224)\n",
    "    images_1 = fn.random_resized_crop(images, size = 224, random_area =[0.14,1] )\n",
    "    images_2 = fn.random_resized_crop(images, size=96, random_area=[0.05,0.14] )\n",
    "    coin = fn.random.coin_flip(probability=0.5 ) #随机镜像对称变换?\n",
    "    images = fn.flip(images, horizontal=coin)\n",
    "    sat = fn.random.uniform(range=[0.2, 1.8]) # 亮度,饱和度,对比度变换的变化范围\n",
    "    hue = fn.random.uniform(range=[-0.2, 0.2]) # 色调 设置hue=0.2\n",
    "    images = fn.color_twist(images, brightness =sat, saturation =sat,contrast =sat, hue=hue)\n",
    "    #颜色变换\n",
    "    images = random_grayscale(images,probability=0.2)\n",
    "#     images = fn.crop_mirror_normalize(\n",
    "#         images,\n",
    "#         device=\"gpu\",\n",
    "#         crop_h=224,\n",
    "#         crop_w=224,\n",
    "#         mean=CIFAR10_MEAN,\n",
    "#         std=CIFAR10_STD)\n",
    "#     transforms.RandomHorizontalFlip(p=0.5),\n",
    "#                 transforms.Compose(color_transform),\n",
    "#                 transforms.ToTensor(),\n",
    "#                 transforms.Normalize(mean=mean, std=std)])\n",
    "#             ] * nmb_crops[i])\n",
    "    return images, labels\n",
    "\n",
    "def random_grayscale(images, probability):\n",
    "    saturate = fn.random.coin_flip(probability=1-probability)\n",
    "    saturate = fn.cast(saturate, dtype=types.FLOAT)\n",
    "    return fn.hsv(images, saturation=saturate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4042ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = get_dali_pipeline(training_data_list[0], training_label_list[0], batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1af8dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6bafd619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义dataloader\n",
    "train_data = DALIGenericIterator(\n",
    "    [pipeline],\n",
    "    ['data', 'label']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a337ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_data):\n",
    "    x, y = data[0]['data'], data[0]['label']\n",
    "    print(f\"size of x is :{x.size()}\")\n",
    "    print(f\"size of y is: {y.size()}\")\n",
    "#     pred = model(x)\n",
    "#     loss = loss_func(pred, y)\n",
    "#     backward(loss, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2e0b7179",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR_INPUT_ITER():\n",
    "    def __init__(self, data, targets, batch_size, train = True):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.batch_size = batch_size\n",
    "        self.train = train\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.i = 0\n",
    "        self.n = len(self.data)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        batch = []\n",
    "        labels = []\n",
    "        for _ in range(self.batch_size):\n",
    "            if self.train and self.i % self.n == 0:\n",
    "                print(\"执行shuffle\")\n",
    "                self.data, self.targets = shuffle(self.data, self.targets, random_state=0)\n",
    "            img, label = self.data[self.i], self.targets[self.i]\n",
    "            batch.append(img)\n",
    "            labels.append(label)\n",
    "            self.i = (self.i + 1) % self.n\n",
    "        return (batch, labels)\n",
    "\n",
    "    next = __next__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "67f7d58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia.dali.pipeline import Pipeline\n",
    "import nvidia.dali.ops as ops\n",
    "import cupy as cp\n",
    "class HybridTrainPipe_CIFAR(Pipeline): # 这个pipeline就相当于是在构造数据集\n",
    "    def __init__(self, images, labels, batch_size, \n",
    "                 num_threads, device_id, data_dir, crop=32, dali_cpu=False, \n",
    "                 local_rank=0,world_size=1,cutout=0):\n",
    "        super(HybridTrainPipe_CIFAR, self).__init__(batch_size, num_threads, device_id, seed=12 + device_id)\n",
    "        self.iterator = iter(CIFAR_INPUT_ITER(images, labels,batch_size ))\n",
    "        dali_device = \"gpu\"\n",
    "        self.input = ops.ExternalSource()\n",
    "        self.input_label = ops.ExternalSource()\n",
    "#         self.image= images\n",
    "#         self.label = labels\n",
    "        self.pad = ops.Paste(device=dali_device, ratio=1.25, fill_value=0)\n",
    "        self.uniform = ops.Uniform(range=(0., 1.))\n",
    "        self.crop = ops.Crop(device=dali_device, crop_h=crop, crop_w=crop)\n",
    "        self.cmnp = ops.CropMirrorNormalize(device=dali_device,\n",
    "                                            dtype=types.FLOAT,\n",
    "                                            output_layout=types.NCHW,\n",
    "                                            image_type=types.RGB,\n",
    "                                            mean=[0.49139968 * 255., 0.48215827 * 255., 0.44653124 * 255.],\n",
    "                                            std=[0.24703233 * 255., 0.24348505 * 255., 0.26158768 * 255.]\n",
    "                                            )\n",
    "        self.coin = ops.CoinFlip(device=dali_device, probability=0.5)\n",
    "        self.rrc_1 = ops.RandomResizedCrop(device=dali_device, size = 224, random_area =[0.14,1] )\n",
    "        self.rrc_2 = ops.RandomResizedCrop(device=dali_device, size=96, random_area=[0.05,0.14] )\n",
    "        self.flip = ops.Flip(device=dali_device, horizontal=self.coin())\n",
    "        self.sat = ops.random.Uniform(device=dali_device,range=[0.2, 1.8]) # 亮度,饱和度,对比度变换的变化范围\n",
    "        self.hue = ops.random.Uniform(device=dali_device,range=[-0.2, 0.2]) # 色调 设置hue=0.2\n",
    "        self.ColorJitter = ops.ColorTwist(device=dali_device, \n",
    "                                          brightness =self.sat(), \n",
    "                                          saturation =self.sat(),contrast =self.sat(), hue=self.hue())\n",
    "        \n",
    "        \n",
    "        self.hsv = self.random_grayscale(probability=0.2)\n",
    "\n",
    "    def iter_setup(self):\n",
    "        (images, labels) = self.iterator.next()\n",
    "        self.feed_input(self.jpegs, images, layout=\"HWC\")\n",
    "        self.feed_input(self.labels, labels)\n",
    "\n",
    "    def define_graph(self): #这个函数是定义怎么对图像进行变换.\n",
    "        rng = self.coin()\n",
    "        self.images = self.input()\n",
    "        self.labels = self.input_label()\n",
    "        output = self.images.gpu()\n",
    "        output = self.rrc_1(output)\n",
    "#         output = self.crop(output, crop_pos_x=self.uniform(), crop_pos_y=self.uniform())\n",
    "        output = self.flip(output)\n",
    "#         output = self.cmnp(output, mirror=rng)\n",
    "        output = self.ColorJitter(output)\n",
    "        return [output, self.labels]\n",
    "                                          \n",
    "    def random_grayscale(self, probability):\n",
    "        saturate = ops.random.CoinFlip(device = \"gpu\",probability=1-probability)\n",
    "        saturate = ops.Cast(device = \"gpu\", dtype=types.FLOAT)(saturate())\n",
    "        hsv = ops.Hsv(device = \"gpu\", saturation=saturate)\n",
    "        return hsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2bd6ce0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24411/2025422904.py:18: DeprecationWarning: The argument ``image_type`` is no longer used and will be removed in a future release.\n",
      "  self.cmnp = ops.CropMirrorNormalize(device=dali_device,\n",
      "/root/enter/envs/fedprocon/lib/python3.9/site-packages/nvidia/dali/ops.py:653: DeprecationWarning: WARNING: `coin_flip` is now deprecated. Use `random.coin_flip` instead.\n",
      "  op_instances.append(_OperatorInstance(input_set, self, **kwargs))\n"
     ]
    }
   ],
   "source": [
    "IMG_DIR = '../data'\n",
    "TRAIN_BS = 256\n",
    "TEST_BS = 200\n",
    "NUM_WORKERS = 4\n",
    "CROP_SIZE = 32\n",
    "\n",
    "\n",
    "pip_train = HybridTrainPipe_CIFAR(images = training_data_list[0], \n",
    "                                  labels = training_label_list[0],\n",
    "                                  batch_size=TRAIN_BS, \n",
    "                                  num_threads=NUM_WORKERS, \n",
    "                                  device_id=0, \n",
    "                                  data_dir=IMG_DIR, \n",
    "                                  crop=CROP_SIZE, \n",
    "                                  world_size=1, \n",
    "                                  local_rank=0, \n",
    "                                  cutout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6f321f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DALIDataloader(DALIGenericIterator):\n",
    "    def __init__(self, pipeline, size, batch_size, output_map=[\"data\", \"label\"], auto_reset=True, onehot_label=False):\n",
    "        super(DALIDataloader, self).__init__(pipelines=pipeline, size=size, auto_reset=auto_reset, output_map=output_map)\n",
    "#         self.size = size #这一步赋值不能执行,不知道为什么...\n",
    "        self.batch_size = batch_size\n",
    "        self.onehot_label = onehot_label\n",
    "        self.output_map = output_map #output_map是指定batch中的字典的两个key\n",
    "        \n",
    "    def __next__(self):\n",
    "        if self._first_batch is not None: # 只有第一个batch走这里\n",
    "            batch = self._first_batch\n",
    "            self._first_batch = None\n",
    "            pdb.set_trace()\n",
    "            return batch[0]  #batch是一个list, list中有一个dict.\n",
    "        # batch==[{'data': tensor([batch_size, channel_size, input_size, input_size]), \n",
    "        #               'labels': tensor([batch_size, 1])}]\n",
    "        data = super().__next__()[0]\n",
    "        \n",
    "        if self.onehot_label:\n",
    "            data[self.output_map[1]] = data[self.output_map[1]].squeeze().long()\n",
    "#             return [data[self.output_map[0]], data[self.output_map[1]].squeeze().long()]\n",
    "#         else:\n",
    "#             return [data[self.output_map[0]], data[self.output_map[1]]]\n",
    "        return data\n",
    "    \n",
    "    def __len__(self): #计算batch的数量\n",
    "        if self._size % self.batch_size==0: \n",
    "         #self._size是调用DALIGenericIterator类的属性,该值的大小等于训练集的样本数量\n",
    "            return self._size // self.batch_size\n",
    "        else:\n",
    "            return self._size // self.batch_size+1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a535336a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/enter/envs/fedprocon/lib/python3.9/site-packages/nvidia/dali/plugin/base_iterator.py:191: Warning: Please set `reader_name` and don't set last_batch_padded and size manually whenever possible. This may lead, in some situations, to missing some samples or returning duplicated ones. Check the Sharding section of the documentation for more details.\n",
      "  _iterator_deprecation_warning()\n",
      "/root/enter/envs/fedprocon/lib/python3.9/site-packages/nvidia/dali/ops.py:653: DeprecationWarning: WARNING: `coin_flip` is now deprecated. Use `random.coin_flip` instead.\n",
      "  op_instances.append(_OperatorInstance(input_set, self, **kwargs))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "when calling operator RandomResizedCrop:\nInput 0 is neither a DALI `DataNode` nor a list of data nodes but `ndarray`.\nAttempt to convert it to a constant node failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/enter/envs/fedprocon/lib/python3.9/site-packages/nvidia/dali/ops.py:1198\u001b[0m, in \u001b[0;36m_preprocess_inputs\u001b[0;34m(inputs, op_name, device, schema)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1198\u001b[0m     inp \u001b[38;5;241m=\u001b[39m \u001b[43m_Constant\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m~/enter/envs/fedprocon/lib/python3.9/site-packages/nvidia/dali/types.py:577\u001b[0m, in \u001b[0;36mConstant\u001b[0;34m(value, dtype, shape, layout, device, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    572\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (_is_compatible_array_type(value) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_true_scalar(value))\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m))\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_scalar_shape(shape)\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m kwargs\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m layout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mConstantNode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/enter/envs/fedprocon/lib/python3.9/site-packages/nvidia/dali/types.py:504\u001b[0m, in \u001b[0;36mConstantNode\u001b[0;34m(device, value, dtype, shape, layout, **kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DALIDataType\u001b[38;5;241m.\u001b[39mFLOAT\n\u001b[0;32m--> 504\u001b[0m actual_type \u001b[38;5;241m=\u001b[39m \u001b[43m_type_from_value_or_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/enter/envs/fedprocon/lib/python3.9/site-packages/nvidia/dali/types.py:493\u001b[0m, in \u001b[0;36mConstantNode.<locals>._type_from_value_or_list\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 493\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected type: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(x)))\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_floats:\n",
      "\u001b[0;31mTypeError\u001b[0m: Unexpected type: <class 'cupy.ndarray'>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m CIFAR_IMAGES_NUM_TRAIN \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50000\u001b[39m\n\u001b[0;32m----> 2\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDALIDataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpip_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                              \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCIFAR_IMAGES_NUM_TRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAIN_BS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                              \u001b[49m\u001b[43monehot_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[100], line 3\u001b[0m, in \u001b[0;36mDALIDataloader.__init__\u001b[0;34m(self, pipeline, size, batch_size, output_map, auto_reset, onehot_label)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pipeline, size, batch_size, output_map\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m], auto_reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, onehot_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m----> 3\u001b[0m         \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDALIDataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpipelines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_reset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_reset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#         self.size = size #这一步赋值不能执行,不知道为什么...\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m batch_size\n",
      "File \u001b[0;32m~/enter/envs/fedprocon/lib/python3.9/site-packages/nvidia/dali/plugin/pytorch.py:181\u001b[0m, in \u001b[0;36mDALIGenericIterator.__init__\u001b[0;34m(self, pipelines, output_map, size, reader_name, auto_reset, fill_last_batch, dynamic_shape, last_batch_padded, last_batch_policy, prepare_first_batch)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_categories \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(output_map)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_map \u001b[38;5;241m=\u001b[39m output_map\n\u001b[0;32m--> 181\u001b[0m \u001b[43m_DaliBaseIterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpipelines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m                           \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mreader_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mauto_reset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mfill_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlast_batch_padded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlast_batch_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mprepare_first_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepare_first_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_first_batch:\n",
      "File \u001b[0;32m~/enter/envs/fedprocon/lib/python3.9/site-packages/nvidia/dali/plugin/base_iterator.py:198\u001b[0m, in \u001b[0;36m_DaliBaseIterator.__init__\u001b[0;34m(self, pipelines, size, reader_name, auto_reset, fill_last_batch, last_batch_padded, last_batch_policy, prepare_first_batch)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipes:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39m_check_api_type_scope(types\u001b[38;5;241m.\u001b[39mPipelineAPIType\u001b[38;5;241m.\u001b[39mITERATOR):\n\u001b[0;32m--> 198\u001b[0m         \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader_name \u001b[38;5;241m=\u001b[39m reader_name\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_from_reader_and_validate()\n",
      "File \u001b[0;32m~/enter/envs/fedprocon/lib/python3.9/site-packages/nvidia/dali/pipeline.py:815\u001b[0m, in \u001b[0;36mPipeline.build\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_threads \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline created with `num_threads` < 1 can only be used \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    813\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor serialization.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 815\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_py_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend_prepared:\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_pipeline_backend()\n",
      "File \u001b[0;32m~/enter/envs/fedprocon/lib/python3.9/site-packages/nvidia/dali/pipeline.py:798\u001b[0m, in \u001b[0;36mPipeline.start_py_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;124;03mStart Python workers (that will run ``ExternalSource`` callbacks).\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;124;03mYou need to call :meth:`start_py_workers` before you call any functionality that creates\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;124;03mwhen calling :meth:`build`.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_py_graph_built:\n\u001b[0;32m--> 798\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_py_pool_started:\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_py_workers()\n",
      "File \u001b[0;32m~/enter/envs/fedprocon/lib/python3.9/site-packages/nvidia/dali/pipeline.py:629\u001b[0m, in \u001b[0;36mPipeline._build_graph\u001b[0;34m(self, define_graph)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 629\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdefine_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    631\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(outputs)\n",
      "Cell \u001b[0;32mIn[104], line 48\u001b[0m, in \u001b[0;36mHybridTrainPipe_CIFAR.define_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#         self.images = self.input()\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#         self.labels = self.input_label()\u001b[39;00m\n\u001b[1;32m     47\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage\n\u001b[0;32m---> 48\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrrc_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#         output = self.crop(output, crop_pos_x=self.uniform(), crop_pos_y=self.uniform())\u001b[39;00m\n\u001b[1;32m     50\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflip(output)\n",
      "File \u001b[0;32m~/enter/envs/fedprocon/lib/python3.9/site-packages/nvidia/dali/ops.py:646\u001b[0m, in \u001b[0;36mpython_op_factory.<locals>.Operator.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_schema_num_inputs(inputs)\n\u001b[0;32m--> 646\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43m_preprocess_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m     input_sets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_input_sets(inputs)\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# Create OperatorInstance for every input set\u001b[39;00m\n",
      "File \u001b[0;32m~/enter/envs/fedprocon/lib/python3.9/site-packages/nvidia/dali/ops.py:1200\u001b[0m, in \u001b[0;36m_preprocess_inputs\u001b[0;34m(inputs, op_name, device, schema)\u001b[0m\n\u001b[1;32m   1198\u001b[0m                     inp \u001b[38;5;241m=\u001b[39m _Constant(inp, device\u001b[38;5;241m=\u001b[39minput_device)\n\u001b[1;32m   1199\u001b[0m                 \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m-> 1200\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mwhen calling operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is neither a DALI `DataNode` nor a list of data nodes but `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(inp)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;124mAttempt to convert it to a constant node failed.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inp, _DataNode):\n\u001b[1;32m   1205\u001b[0m                 inp \u001b[38;5;241m=\u001b[39m nvidia\u001b[38;5;241m.\u001b[39mdali\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39m_instantiate_constant_node(input_device, inp)\n",
      "\u001b[0;31mTypeError\u001b[0m: when calling operator RandomResizedCrop:\nInput 0 is neither a DALI `DataNode` nor a list of data nodes but `ndarray`.\nAttempt to convert it to a constant node failed."
     ]
    }
   ],
   "source": [
    "CIFAR_IMAGES_NUM_TRAIN = 50000\n",
    "train_loader = DALIDataloader(pipeline=pip_train, \n",
    "                              size=CIFAR_IMAGES_NUM_TRAIN, \n",
    "                              batch_size=TRAIN_BS, \n",
    "                              onehot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "720b7ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExternalInputIterator(object):\n",
    "    def __init__(self, batch_size):\n",
    "        self.images_dir = \"../../data/images/\"\n",
    "        self.batch_size = batch_size\n",
    "        with open(self.images_dir + \"file_list.txt\", 'r') as f:\n",
    "            self.files = [line.rstrip() for line in f if line != '']\n",
    "        shuffle(self.files)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.i = 0\n",
    "        self.n = len(self.files)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        batch = []\n",
    "        labels = []\n",
    "        for _ in range(self.batch_size):\n",
    "            jpeg_filename, label = self.files[self.i].split(' ')\n",
    "            f = open(self.images_dir + jpeg_filename, 'rb')\n",
    "            batch.append(np.frombuffer(f.read(), dtype = np.uint8))\n",
    "            labels.append(np.array([label], dtype = np.uint8))\n",
    "            self.i = (self.i + 1) % self.n\n",
    "        return (batch, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedprocon",
   "language": "python",
   "name": "fedprocon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
